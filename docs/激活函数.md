### 1. **Tanh（双曲正切函数）**
- **公式**：
  $$
  \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
  $$
- **含义**：
  - 输出范围在 **(-1, 1)** 之间，是**零中心化**的（均值为0），有利于梯度下降优化。
  - 形状为“S”型，两端饱和（梯度趋于0），中间近似线性。
  - 常用于**隐藏层**，比 Sigmoid 在训练中表现更好，但仍存在梯度消失问题。

---

### 2. **ReLU（线性整流函数）**
- **公式**：
  $$
  \text{ReLU}(x) = \max(0, x)
  $$
- **含义**：
  - 当 $ x > 0 $ 时输出 $ x $，否则输出 0。
  - **计算简单、高效**，能缓解梯度消失问题（正区间梯度恒为1）。
  - 缺点：存在“神经元死亡”问题（负区间梯度为0，导致权重无法更新）。

---

### 3. **Sigmoid（S型函数）**
- **公式**：
  $$
  \sigma(x) = \frac{1}{1 + e^{-x}}
  $$
- **含义**：
  - 输出范围在 **(0, 1)** 之间，常用于**二分类输出层**（表示概率）。
  - 两端梯度趋于0，容易引起梯度消失，训练较慢。
  - 在深层网络中已较少用于隐藏层。

---

### 4. **Leaky ReLU（带泄漏的ReLU）**
- **公式**：
  $$
  \text{LeakyReLU}(x) = \max(\alpha x, x)
  $$
  其中 $\alpha$ 为小正数（如 0.01）。
- **含义**：
  - 在负区间引入一个小的斜率 $\alpha$，避免神经元死亡。
  - 保留了 ReLU 的高效性，同时缓解了梯度为零的问题。

---

### 5. **Swish（自门控激活函数）**
- **公式**：
  $$
  \text{Swish}(x) = x \cdot \sigma(\beta x)
  $$
  其中 $\sigma$ 是 Sigmoid 函数，$\beta$ 是可学习参数或固定值（常取1）。
- **含义**：
  - 结合了**线性与非线性**特性，在负区间非零，平滑且非单调。
  - 实验表明在某些任务中优于 ReLU，尤其在深层网络中表现稳定。
  - 计算量略大，但预测精度通常较高（如表中的64%）。

---

### 总结对比：
| 函数        | 输出范围     | 优点                          | 缺点                     |
|-------------|--------------|-------------------------------|--------------------------|
| Tanh        | (-1, 1)      | 零中心化，梯度稳定            | 两端饱和，梯度消失       |
| ReLU        | [0, +∞)      | 计算快，缓解梯度消失          | 神经元死亡               |
| Sigmoid     | (0, 1)       | 输出概率，适合二分类          | 梯度消失严重             |
| Leaky ReLU  | (-∞, +∞)     | 缓解死亡神经元问题            | 需调参α                  |
| Swish       | (-∞, +∞)     | 平滑，自适应，高精度          | 计算稍复杂               |

这些激活函数在神经网络中负责引入非线性，使得网络能够拟合复杂函数。选择哪种函数需结合任务特性、网络深度和计算效率综合考虑。