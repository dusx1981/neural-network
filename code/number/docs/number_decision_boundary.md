# MNIST 神经网络决策边界与数学原理详解

本文档详细阐述 `code/number/number.py` 中神经网络学习的决策边界及其背后的数学原理。

---

## 一、决策边界的基本概念

### 1.1 什么是决策边界？

**决策边界（Decision Boundary）** 是分类器在特征空间中划分不同类别的分界线（面）。对于神经网络而言，决策边界是由网络参数定义的非线性超平面。

### 1.2 MNIST 问题的决策空间

```
输入空间: ℝ⁷⁸⁴  (784维，对应28×28图像)
    ↓
特征空间: ℝ²⁵⁶  (256维，隐藏层输出)
    ↓
决策空间: 10个类别的划分区域
```

**关键理解**：虽然输入是 784 维，但决策边界存在于网络学习到的特征表示空间中。

---

## 二、网络架构的数学表示

### 2.1 完整的前向传播公式

给定输入图像 **x** ∈ ℝ⁷⁸⁴，网络计算如下：

#### 第一层（隐藏层）

```
z₁ = W₁x + b₁
h = ReLU(z₁) = max(0, z₁)
```

其中：
- **W₁** ∈ ℝ²⁵⁶ˣ⁷⁸⁴：第一层的权重矩阵
- **b₁** ∈ ℝ²⁵⁶：第一层的偏置向量
- **z₁** ∈ ℝ²⁵⁶：线性变换后的预激活值
- **h** ∈ ℝ²⁵⁶：ReLU 激活后的隐藏特征

#### 第二层（输出层）

```
z₂ = W₂h + b₂
ŷ = softmax(z₂)
```

其中：
- **W₂** ∈ ℝ¹⁰ˣ²⁵⁶：第二层的权重矩阵
- **b₂** ∈ ℝ¹⁰：第二层的偏置向量
- **z₂** ∈ ℝ¹⁰：logits（原始分数）
- **ŷ** ∈ ℝ¹⁰：各类别的预测概率

### 2.2 softmax 函数详解

```
softmax(z₂ᵢ) = exp(z₂ᵢ) / Σⱼexp(z₂ⱼ)
```

**作用**：
- 将 logits 转换为概率分布
- 所有类别概率之和为 1
- 放大概率差异，使最大值的类别更突出

---

## 三、决策边界的形成机制

### 3.1 单神经元：二元决策边界

考虑隐藏层中的一个神经元 **hᵢ**：

```
hᵢ = max(0, w₁ᵢ·x + b₁ᵢ)
```

**激活条件**：
```
w₁ᵢ·x + b₁ᵢ > 0
```

这是一个**线性不等式**，定义了一个半空间：
- 满足条件的输入 → 神经元激活（hᵢ > 0）
- 不满足条件的输入 → 神经元抑制（hᵢ = 0）

**几何解释**：

```
                    w₁ᵢ·x + b₁ᵢ = 0
                         │
    hᵢ = 0 (抑制)         │         hᵢ > 0 (激活)
         ○               │            ●
         ○               │            ●
    ─────○───────────────┼────────────●─────
         ○               │            ●
         ○               │            ●
                         ▼
                   法向量 w₁ᵢ
```

### 3.2 ReLU 网络：分段线性决策边界

#### 关键定理

带有 ReLU 激活的多层神经网络定义的是**分段线性决策边界**。

**原因**：ReLU 是分段线性函数，线性变换的组合仍是线性的。

#### 单隐藏层网络的分段线性特性

对于给定的输入 **x**，定义激活模式 **a** ∈ {0,1}²⁵⁶：

```
aᵢ = 1  if w₁ᵢ·x + b₁ᵢ > 0  (激活)
aᵢ = 0  otherwise            (抑制)
```

则隐藏层输出可表示为：

```
h = diag(a)(W₁x + b₁)
```

其中 diag(a) 是以 a 为对角元素的对角矩阵。

输出层计算变为：

```
z₂ = W₂·diag(a)·(W₁x + b₁) + b₂
   = W₂·diag(a)·W₁x + W₂·diag(a)·b₁ + b₂
   = W_eff(a)·x + b_eff(a)
```

其中 **W_eff(a)** 和 **b_eff(a)** 依赖于激活模式 **a**。

**结论**：
- 每个激活模式 **a** 对应一个线性区域
- 在该区域内，网络等价于一个线性分类器
- 不同区域有不同的有效权重 **W_eff**

### 3.3 决策边界可视化（简化示例）

考虑二维输入空间中的二分类问题：

```
                    决策边界
    ┌─────────────────────────────────────┐
    │  区域 1      │       区域 2         │
    │   h₁>0      │        h₁≤0          │
    │   h₂>0      │        h₂>0          │
    │    ●●●      │        ○○○           │
    │    ●●●      │        ○○○           │
    │─────────────┼──────────────────────│
    │  区域 3      │       区域 4         │
    │   h₁≤0      │        h₁≤0          │
    │   h₂≤0      │        h₂≤0          │
    │    ○○○      │        ○○○           │
    └─────────────────────────────────────┘
```

**实际 MNIST 的情况**：
- 784 维输入空间
- 256 个 ReLU 神经元
- 理论上最多 2²⁵⁶ 个线性区域
- 实际激活区域取决于数据分布

---

## 四、多分类决策边界

### 4.1 一对多（One-vs-Rest）决策边界

对于 10 个数字类别，网络学习 10 个决策函数：

```
fᵢ(x) = z₂ᵢ = w₂ᵢ·h + b₂ᵢ    (i = 0,1,...,9)
```

**分类规则**：
```
predicted_class = argmaxᵢ fᵢ(x)
```

### 4.2 类别间的决策边界

类别 **i** 和类别 **j** 之间的决策边界定义为：

```
{x : fᵢ(x) = fⱼ(x) > fₖ(x) for all k ≠ i,j}
```

即满足以下条件的输入集合：

```
(w₂ᵢ - w₂ⱼ)·h + (b₂ᵢ - b₂ⱼ) = 0
```

这是隐藏空间中的一个超平面！

**重要洞察**：
- 隐藏层将输入映射到适合线性分离的特征空间
- 在 256 维隐藏空间中，10 个类别被线性超平面分离
- 这解释了为什么需要隐藏层：将非线性可分问题转换为线性可分问题

### 4.3 决策区域示例

考虑三个类别（简化说明）：

```
        隐藏空间 (h₁, h₂)
    
    f₁>f₂, f₁>f₃ │ f₂>f₁, f₂>f₃
         ●        │        ○
         ●   ─────┼─────   ○
    ─────●────────┼────────○─────
         ●        │        ○
         ●        │        ○
    ─────●────────┼────────○─────
         ●        │        ○
                  │
    f₃>f₁, f₃>f₂ │
         ★
         ★
```

三个类别的决策边界是三条直线的交点形成的区域划分。

---

## 五、损失函数与决策边界优化

### 5.1 交叉熵损失函数

```
L = -Σᵢ yᵢ log(ŷᵢ)
```

其中：
- **y** ∈ {0,1}¹⁰：one-hot 编码的真实标签
- **ŷ** ∈ (0,1)¹⁰：softmax 预测概率

对于单个样本（真实类别为 c）：

```
L = -log(ŷ_c) = -log(softmax(z₂_c))
  = -z₂_c + log(Σⱼexp(z₂ⱼ))
```

### 5.2 损失函数的几何解释

**优化目标**：最大化正确类别的 logit，同时抑制其他类别

```
minimize:  log(Σⱼexp(z₂ⱼ)) - z₂_c
```

**决策边界的演化**：

1. **训练初期**：随机权重，决策边界混乱
   ```
   损失值：~2.3 (随机猜测，ln(10) ≈ 2.3)
   准确率：~10%
   ```

2. **训练中期**：决策边界开始形成
   ```
   损失值：~0.5
   准确率：~85%
   ```

3. **训练后期**：决策边界清晰
   ```
   损失值：~0.05
   准确率：~98%
   ```

### 5.3 边界间隔（Margin）

**正确分类的条件**：

```
z₂_c > z₂ⱼ + margin    for all j ≠ c
```

其中 margin > 0 是决策边界到最近样本的距离。

**大间隔的好处**：
- 更好的泛化能力
- 对噪声更鲁棒
- 决策边界更"清晰"

---

## 六、具体数值示例

### 6.1 简化的 2D 示例

假设我们有一个极简网络处理 2D 输入：

```
输入: x = [x₁, x₂]

Layer1: h = ReLU(W₁x + b₁)
W₁ = [[1, -1],
      [-1, 1],
      [1, 1]]
b₁ = [0, 0, -1]

Layer2: z = W₂h + b₂
W₂ = [[1, 0, 0],
      [0, 1, 0]]
b₂ = [0, 0]
```

**计算过程**：

对于输入 x = [0.5, 0.3]：

```
Layer1 线性部分:
W₁x = [1×0.5 + (-1)×0.3, (-1)×0.5 + 1×0.3, 1×0.5 + 1×0.3 - 1]
    = [0.2, -0.2, -0.2]

ReLU 激活:
h = [max(0, 0.2), max(0, -0.2), max(0, -0.2)]
  = [0.2, 0, 0]

Layer2:
z = [1×0.2 + 0×0 + 0×0, 0×0.2 + 1×0 + 0×0]
  = [0.2, 0]

Softmax:
ŷ = [exp(0.2)/(exp(0.2)+exp(0)), exp(0)/(exp(0.2)+exp(0))]
  ≈ [0.55, 0.45]

预测: 类别 0 (概率 0.55)
```

**决策边界分析**：

激活模式取决于输入位置：

```
           x₂
           │
    h₁>0   │   h₁>0
    h₂≤0   │   h₂>0
           │
    ───────┼─────── x₁
           │
    h₁≤0   │   h₁≤0
    h₂>0   │   h₂≤0
           │
```

分界线：
- h₁ 激活边界：x₁ - x₂ = 0 （直线 x₁ = x₂）
- h₂ 激活边界：-x₁ + x₂ = 0 （直线 x₁ = x₂）
- h₃ 激活边界：x₁ + x₂ - 1 = 0 （直线 x₁ + x₂ = 1）

### 6.2 MNIST 的实际决策边界

考虑数字 "0" vs "1" 的二分类子问题：

**训练后的权重学习**：

```
Layer2 对数字 0 的权重（部分示例）:
w₂₀ = [0.12, -0.05, 0.23, ..., -0.08]  (256维)

Layer2 对数字 1 的权重（部分示例）:
w₂₁ = [-0.08, 0.31, -0.15, ..., 0.19]  (256维)
```

**决策边界超平面**：

```
(w₂₀ - w₂₁)·h + (b₂₀ - b₂₁) = 0

[0.20, -0.36, 0.38, ..., -0.27]·h + 0.5 = 0
```

**物理意义**：
- 正权重对应的隐藏神经元：如果激活，倾向于分类为 "0"
- 负权重对应的隐藏神经元：如果激活，倾向于分类为 "1"
- 决策边界平衡了这些证据

---

## 七、决策边界的复杂度分析

### 7.1 线性区域数量

理论上线性区域的数量上界：

```
#regions ≤ Σᵢ₌₀ᵈ C(n, i)

其中：
- d: 输入维度
- n: 隐藏层神经元数量
- C(n,i): 组合数
```

对于本网络（d=784, n=256）：
- 理论上最多 2²⁵⁶ 个区域
- 实际激活模式受限于训练数据

### 7.2 表达能力

**通用逼近定理**：

具有足够多隐藏神经元的单隐藏层神经网络可以以任意精度逼近任何连续函数。

**本网络的表达能力**：

- 256 个 ReLU 神经元足够表示复杂的决策边界
- 可以学习非线性的、多模态的分类边界
- 对于 MNIST 任务，表达能力有余量

---

## 八、可视化决策边界（概念性）

### 8.1 使用 PCA 降维可视化

由于 784 维无法直接可视化，我们可以使用 PCA 降到 2D：

```
原始空间: x ∈ ℝ⁷⁸⁴
    ↓ PCA
低维空间: x' ∈ ℝ²
    ↓ 神经网络
预测: ŷ ∈ ℝ¹⁰
```

**概念图**：

```
        PCA 降维后的 2D 空间
    
    ╔══════════════════════════════════╗
    ║  区域 0      │       区域 1      ║
    ║    ○○○      │        ●●●        ║
    ║   ○○○○      │       ●●●●        ║
    ║  ───────────┼────────────────   ║
    ║      ╲      │      ╱            ║
    ║       ╲     │     ╱             ║
    ║   区域 8    │    区域 3          ║
    ║      △△     │    □□□            ║
    ║     △△△     │   □□□□            ║
    ╚══════════════════════════════════╝
    
    注：实际边界更复杂，此处为简化示意
```

### 8.2 t-SNE 可视化

t-SNE 可以更好地保持局部结构：

```
相同数字的样本在特征空间中聚类
不同数字的样本被决策边界分开
```

---

## 九、与其他分类器的对比

### 9.1 逻辑回归（无隐藏层）

```
z = Wx + b
ŷ = softmax(z)
```

**决策边界**：单一超平面（线性）

**限制**：无法处理非线性可分数据

### 9.2 本网络（单隐藏层）

```
h = ReLU(W₁x + b₁)
z = W₂h + b₂
```

**决策边界**：分段线性

**优势**：
- 可以学习复杂的非线性边界
- 隐藏层将数据映射到线性可分的特征空间

### 9.3 深度网络（多隐藏层）

```
h₁ = ReLU(W₁x + b₁)
h₂ = ReLU(W₂h₁ + b₂)
...
z = Wₙhₙ₋₁ + bₙ
```

**决策边界**：更复杂的分段线性

**优势**：
- 层次化特征学习
- 指数级增长的线性区域

---

## 十、数学原理总结

### 10.1 核心公式

**前向传播**：
```
h = ReLU(W₁x + b₁)
z = W₂h + b₂
ŷ = softmax(z)
```

**损失函数**：
```
L = -Σᵢ yᵢ log(ŷᵢ)
```

**决策边界**：
```
{x : zᵢ = zⱼ > zₖ for all k ≠ i,j}
```

**激活模式**：
```
a = 𝟙(W₁x + b₁ > 0)
z = W₂·diag(a)·W₁x + const
```

### 10.2 关键洞察

1. **非线性映射**：ReLU 将输入空间划分为多个线性区域

2. **特征变换**：隐藏层学习将输入映射到线性可分的特征空间

3. **线性分类**：输出层在特征空间中进行线性分类

4. **端到端学习**：特征提取和分类边界同时优化

5. **分段线性**：整个网络的决策边界是分段线性的

---

## 十一、训练动态与决策边界演化

### 11.1 训练过程可视化

```
Epoch 0:
  损失: 2.30
  边界: 随机混乱，无明显结构
  
Epoch 3:
  损失: 0.80
  边界: 开始形成基本结构，大致分离数字组
  
Epoch 6:
  损失: 0.25
  边界: 清晰分离主要类别，细节待优化
  
Epoch 10:
  损失: 0.05
  边界: 精细调整，处理边界样本
```

### 11.2 边界优化机制

**Adam 优化器更新**：

```
m_t = β₁·m_{t-1} + (1-β₁)·g_t        # 一阶矩估计（动量）
v_t = β₂·v_{t-1} + (1-β₂)·g_t²       # 二阶矩估计

m̂_t = m_t / (1-β₁ᵗ)                  # 偏差修正
v̂_t = v_t / (1-β₂ᵗ)

θ_t = θ_{t-1} - α·m̂_t / (√v̂_t + ε)   # 参数更新
```

其中 **g_t** 是损失函数对参数的梯度，指导决策边界的调整方向。

---

## 十二、结论

### 12.1 本网络的决策边界特性

1. **分段线性**：由 256 个 ReLU 神经元产生的分段线性边界
2. **非线性可分**：能够处理复杂的 MNIST 分类任务
3. **特征空间线性**：在 256 维隐藏空间中，类别是线性可分的
4. **数据驱动**：边界形状完全由训练数据决定

### 12.2 数学本质

神经网络通过以下方式学习决策边界：

1. **非线性映射**：f: ℝ⁷⁸⁴ → ℝ²⁵⁶，将输入映射到特征空间
2. **线性分类**：在特征空间中使用线性超平面分离类别
3. **组合优化**：损失函数引导边界向最小化分类错误的方向调整

### 12.3 实用意义

理解决策边界有助于：

- **模型诊断**：分析错误分类的样本位于边界的什么位置
- **对抗样本**：理解微小扰动如何跨越决策边界
- **模型改进**：设计更好的架构以学习更优的决策边界
- **可解释性**：理解网络为什么做出特定预测

---

**附录：符号说明**

| 符号 | 含义 | 维度 |
|------|------|------|
| x | 输入图像（展平） | 784 |
| W₁ | 第一层权重 | 256×784 |
| b₁ | 第一层偏置 | 256 |
| h | 隐藏层输出 | 256 |
| W₂ | 第二层权重 | 10×256 |
| b₂ | 第二层偏置 | 10 |
| z | logits（输出前） | 10 |
| ŷ | 预测概率 | 10 |
| y | 真实标签（one-hot） | 10 |
| L | 损失函数 | 标量 |
